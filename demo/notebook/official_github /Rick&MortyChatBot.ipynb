{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BJTFDGR/Adver_Conv/blob/master/Rick%26MortyChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMFOoHOaq96x"
      },
      "source": [
        "# Making a Rick & Morty chat bot for Chai\n",
        "\n",
        "<img src=\"https://i.imgur.com/dtGCncO.png\" width=\"250\">\n",
        "\n",
        "We're going to be making a chatbot, based on Microsoft's DialoGPT. I've seen lots of other guides on training chatbots, but I haven't come across one which actually deploys the bot. This tutorial will cover deploying our new bot to [Chai](https://chai.ml/), a platform for creating and interacting with conversational AI's. It will allow us to chat with our bot through a mobile app, from anywhere, at any time. We will also be able to see performance stats and watch it climb the Chai bot leaderboard.\n",
        "\n",
        "By the end of this tutorial you will have your very own chatbot, like the one pictured above üòé\n",
        "\n",
        "Almost all of the code for training this bot was made by [Mohamed Hassan](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb#scrollTo=THybpQmXhoxK). Their code has been adapted to suit the tutorial better.\n",
        "\n",
        "The training data has been fetched from this [article](https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts/#1.-Data-%F0%9F%93%81) by [Andrada Olteanu](https://www.kaggle.com/andradaolteanu) on [Kaggle](https://www.kaggle.com/)\n",
        "\n",
        "\n",
        "Let's get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EN-zZi6pHIB"
      },
      "source": [
        "### Install the Huggingface transformers module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:03:12.876080Z",
          "iopub.status.busy": "2021-08-07T14:03:12.875672Z",
          "iopub.status.idle": "2021-08-07T14:03:19.269243Z",
          "shell.execute_reply": "2021-08-07T14:03:19.268187Z",
          "shell.execute_reply.started": "2021-08-07T14:03:12.876045Z"
        },
        "id": "WD6iOcTmoaxE",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (4.22.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from transformers) (2022.9.13)\n",
            "Requirement already satisfied: requests in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from transformers) (0.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from requests->transformers) (2022.9.14)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /localscratch/chenboc1/anaconda3/envs/NMT/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\n"
          ]
        }
      ],
      "source": [
        "! /localscratch/chenboc1/anaconda3/envs/NMT/bin/pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXuRTjrJo5vk"
      },
      "source": [
        "## Import DialoGPT\n",
        "DialoGPT is a chatbot model made by microsoft. This will be the base for our RickBot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:03:19.273052Z",
          "iopub.status.busy": "2021-08-07T14:03:19.272773Z",
          "iopub.status.idle": "2021-08-07T14:03:22.767334Z",
          "shell.execute_reply": "2021-08-07T14:03:22.766473Z",
          "shell.execute_reply.started": "2021-08-07T14:03:19.273023Z"
        },
        "id": "FSvzC1j7_Tr8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer,AutoModelWithLMHead, AutoConfig,AutoModelWithLMHead\n",
        "import torch,os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
        "model_size = \"medium\" \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"microsoft/DialoGPT-{model_size}\",cache_dir=\"cached\")\n",
        "model = AutoModelWithLMHead.from_pretrained(f\"microsoft/DialoGPT-{model_size}\",cache_dir=\"cached\")\n",
        "\n",
        "# model_name='microsoft/DialoGPT-medium'\n",
        "# cache_dir=\"cached\"\n",
        "\n",
        "# config = AutoConfig.from_pretrained(model_name, cache_dir=cache_dir)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
        "# model = AutoModelWithLMHead.from_pretrained(\n",
        "#     model_name,\n",
        "#     from_tf=False,\n",
        "#     config=config,\n",
        "#     cache_dir=cache_dir,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45l8_zjlpD5B"
      },
      "source": [
        "## Chat with the untrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:03:22.769538Z",
          "iopub.status.busy": "2021-08-07T14:03:22.769187Z",
          "iopub.status.idle": "2021-08-07T14:03:36.769613Z",
          "shell.execute_reply": "2021-08-07T14:03:36.768651Z",
          "shell.execute_reply.started": "2021-08-07T14:03:22.769502Z"
        },
        "id": "7NaCfs94pLw4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type \"q\" to quit. Automatically quits after 5 messages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/localscratch/chenboc1/anaconda3/envs/py36/lib/python3.6/site-packages/transformers/modeling_gpt2.py:148: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272155627/work/aten/src/ATen/native/TensorCompare.cpp:328.)\n",
            "  w = torch.where(mask, w, self.masked_bias)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialoGPT: Hey! :D\n",
            "DialoGPT: I'm good, how are you?\n",
            "DialoGPT: I will\n"
          ]
        }
      ],
      "source": [
        "def chat(model, tokenizer, trained=False):\n",
        "    print(\"type \\\"q\\\" to quit. Automatically quits after 5 messages\")\n",
        "\n",
        "    for step in range(5):\n",
        "        message = input(\"MESSAGE: \")\n",
        "\n",
        "        if message in [\"\", \"q\"]:  # if the user doesn't wanna talk\n",
        "            break\n",
        "\n",
        "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "        new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "        # append the new user input tokens to the chat history\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "        # generated a response while limiting the total chat history to 1000 tokens, \n",
        "        if (trained):\n",
        "            chat_history_ids = model.generate(\n",
        "                bot_input_ids, \n",
        "                max_length=1000,\n",
        "                pad_token_id=tokenizer.eos_token_id,  \n",
        "                no_repeat_ngram_size=3,       \n",
        "                do_sample=True, \n",
        "                top_k=100, \n",
        "                top_p=0.7,\n",
        "                temperature = 0.8, \n",
        "            )\n",
        "        else:\n",
        "            chat_history_ids = model.generate(\n",
        "                bot_input_ids, \n",
        "                max_length=1000, \n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=3\n",
        "            )\n",
        "\n",
        "        # pretty print last ouput tokens from bot\n",
        "        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "\n",
        "chat(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIF90ucrhgFo"
      },
      "source": [
        "It's capable of holding a conversation, but doesn't resemble Rick Sanchez at all yet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kj2BIaUiS71"
      },
      "source": [
        "## Configuring the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:03:36.771854Z",
          "iopub.status.busy": "2021-08-07T14:03:36.771359Z",
          "iopub.status.idle": "2021-08-07T14:03:36.785617Z",
          "shell.execute_reply": "2021-08-07T14:03:36.784711Z",
          "shell.execute_reply.started": "2021-08-07T14:03:36.771820Z"
        },
        "id": "jv9TXRvV1HIk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import glob, logging, os, pickle, random, re, torch, pandas as pd, numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from pathlib import Path\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Args to allow for easy convertion of python script to notebook\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.output_dir = f'runs/output-{model_size}'\n",
        "        self.model_type = 'gpt2'\n",
        "        self.model_name_or_path = f'microsoft/DialoGPT-{model_size}'\n",
        "        self.config_name = f'microsoft/DialoGPT-{model_size}'\n",
        "        self.tokenizer_name = f'microsoft/DialoGPT-{model_size}'\n",
        "        self.cache_dir = 'cached'\n",
        "        self.block_size = 512\n",
        "        self.per_gpu_train_batch_size = 4\n",
        "        self.gradient_accumulation_steps = 1\n",
        "        self.learning_rate = 5e-5\n",
        "        self.weight_decay = 0.0\n",
        "        self.adam_epsilon = 1e-8\n",
        "        self.max_grad_norm = 1.0\n",
        "        self.num_train_epochs = 3  # 3\n",
        "        self.max_steps = -1\n",
        "        self.warmup_steps = 0\n",
        "        self.logging_steps = 1000\n",
        "        self.save_total_limit = None\n",
        "        self.seed = 42\n",
        "        self.local_rank = -1\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAES6tb6jh1O"
      },
      "source": [
        "# Gather the training data\n",
        "\n",
        "We're using some rick and morty scripts from this [article](https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts/#1.-Data-%F0%9F%93%81) by [Andrada Olteanu](https://www.kaggle.com/andradaolteanu)  \\(the data can be found [here](https://www.kaggle.com/andradaolteanu/rickmorty-scripts)\\)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n_n_Jo1jq4H"
      },
      "source": [
        "Let's look at the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:03:38.725717Z",
          "iopub.status.busy": "2021-08-07T14:03:38.725439Z",
          "iopub.status.idle": "2021-08-07T14:03:38.754588Z",
          "shell.execute_reply": "2021-08-07T14:03:38.753871Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.725675Z"
        },
        "id": "OFZsicHfjpxz",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>context 1</th>\n",
              "      <th>context 2</th>\n",
              "      <th>context 3</th>\n",
              "      <th>context 4</th>\n",
              "      <th>context 5</th>\n",
              "      <th>context 6</th>\n",
              "      <th>context 7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What do you think of this... flying vehicle, M...</td>\n",
              "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
              "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
              "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
              "      <td>It's the middle of the night. What are you tal...</td>\n",
              "      <td>I got a surprise for you, Morty.</td>\n",
              "      <td>What, Rick? What‚Äôs going on?</td>\n",
              "      <td>Morty! You gotta come on. Jus'... you gotta co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
              "      <td>What do you think of this... flying vehicle, M...</td>\n",
              "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
              "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
              "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
              "      <td>It's the middle of the night. What are you tal...</td>\n",
              "      <td>I got a surprise for you, Morty.</td>\n",
              "      <td>What, Rick? What‚Äôs going on?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Morty. I had to... I had to do it. I had‚Äî I ha...</td>\n",
              "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
              "      <td>What do you think of this... flying vehicle, M...</td>\n",
              "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
              "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
              "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
              "      <td>It's the middle of the night. What are you tal...</td>\n",
              "      <td>I got a surprise for you, Morty.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What?! A bomb?!</td>\n",
              "      <td>Morty. I had to... I had to do it. I had‚Äî I ha...</td>\n",
              "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
              "      <td>What do you think of this... flying vehicle, M...</td>\n",
              "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
              "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
              "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
              "      <td>It's the middle of the night. What are you tal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>We're gonna drop it down there just get a whol...</td>\n",
              "      <td>What?! A bomb?!</td>\n",
              "      <td>Morty. I had to... I had to do it. I had‚Äî I ha...</td>\n",
              "      <td>Yeah, Rick... I-it's great. Is this the surprise?</td>\n",
              "      <td>What do you think of this... flying vehicle, M...</td>\n",
              "      <td>We gotta go, gotta get outta here, come on. Go...</td>\n",
              "      <td>Ow! Ow! You're tugging me too hard!</td>\n",
              "      <td>Come on, I got a surprise for you.  Come on, h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            response  \\\n",
              "0  What do you think of this... flying vehicle, M...   \n",
              "1  Yeah, Rick... I-it's great. Is this the surprise?   \n",
              "2  Morty. I had to... I had to do it. I had‚Äî I ha...   \n",
              "3                                    What?! A bomb?!   \n",
              "4  We're gonna drop it down there just get a whol...   \n",
              "\n",
              "                                           context 1  \\\n",
              "0  We gotta go, gotta get outta here, come on. Go...   \n",
              "1  What do you think of this... flying vehicle, M...   \n",
              "2  Yeah, Rick... I-it's great. Is this the surprise?   \n",
              "3  Morty. I had to... I had to do it. I had‚Äî I ha...   \n",
              "4                                    What?! A bomb?!   \n",
              "\n",
              "                                           context 2  \\\n",
              "0                Ow! Ow! You're tugging me too hard!   \n",
              "1  We gotta go, gotta get outta here, come on. Go...   \n",
              "2  What do you think of this... flying vehicle, M...   \n",
              "3  Yeah, Rick... I-it's great. Is this the surprise?   \n",
              "4  Morty. I had to... I had to do it. I had‚Äî I ha...   \n",
              "\n",
              "                                           context 3  \\\n",
              "0  Come on, I got a surprise for you.  Come on, h...   \n",
              "1                Ow! Ow! You're tugging me too hard!   \n",
              "2  We gotta go, gotta get outta here, come on. Go...   \n",
              "3  What do you think of this... flying vehicle, M...   \n",
              "4  Yeah, Rick... I-it's great. Is this the surprise?   \n",
              "\n",
              "                                           context 4  \\\n",
              "0  It's the middle of the night. What are you tal...   \n",
              "1  Come on, I got a surprise for you.  Come on, h...   \n",
              "2                Ow! Ow! You're tugging me too hard!   \n",
              "3  We gotta go, gotta get outta here, come on. Go...   \n",
              "4  What do you think of this... flying vehicle, M...   \n",
              "\n",
              "                                           context 5  \\\n",
              "0                   I got a surprise for you, Morty.   \n",
              "1  It's the middle of the night. What are you tal...   \n",
              "2  Come on, I got a surprise for you.  Come on, h...   \n",
              "3                Ow! Ow! You're tugging me too hard!   \n",
              "4  We gotta go, gotta get outta here, come on. Go...   \n",
              "\n",
              "                                           context 6  \\\n",
              "0                       What, Rick? What‚Äôs going on?   \n",
              "1                   I got a surprise for you, Morty.   \n",
              "2  It's the middle of the night. What are you tal...   \n",
              "3  Come on, I got a surprise for you.  Come on, h...   \n",
              "4                Ow! Ow! You're tugging me too hard!   \n",
              "\n",
              "                                           context 7  \n",
              "0  Morty! You gotta come on. Jus'... you gotta co...  \n",
              "1                       What, Rick? What‚Äôs going on?  \n",
              "2                   I got a surprise for you, Morty.  \n",
              "3  It's the middle of the night. What are you tal...  \n",
              "4  Come on, I got a surprise for you.  Come on, h...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('data/RickAndMortyScripts.csv')\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de1R3f3-kGwY"
      },
      "source": [
        "\n",
        "We want the model to be aware of previous messages from the dialogue to help it decide what to say next. We call this context. The dataset has context from 7 previous messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gRD9mhKkcT9"
      },
      "source": [
        "### Formatting the data and defining some helper functions\n",
        "We need to construct the data in the right format so the bot can interpret it properly. To do this we're adding special characters like the 'end of string' charater\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:03:38.907102Z",
          "iopub.status.busy": "2021-08-07T14:03:38.906722Z",
          "iopub.status.idle": "2021-08-07T14:03:38.917840Z",
          "shell.execute_reply": "2021-08-07T14:03:38.916753Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.907072Z"
        },
        "id": "mdjT5EqKkwZb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def construct_conv(row, tokenizer, eos = True):\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
        "    conv = flatten(conv)\n",
        "    return conv\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, df_trn):\n",
        "    print(\"load and cache examples is being run ****************************************\")\n",
        "    return ConversationDataset(tokenizer, args, df_trn)\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
        "\n",
        "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
        "        directory = args.cache_dir\n",
        "        cached_features_file = os.path.join(directory, args.model_type + \"_cached_lm_\" + str(block_size))\n",
        "\n",
        "        logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "        self.examples = []\n",
        "        for _, row in df.iterrows():\n",
        "            conv = construct_conv(row, tokenizer)\n",
        "            self.examples.append(conv)\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        with open(cached_features_file, \"wb\") as handle:\n",
        "            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item], dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83oT-xHu4msu"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now, this is quite a hefty chunk of code but don't worry you don't need to understand it yet, we can cover this in later tutorials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:03:38.935954Z",
          "iopub.status.busy": "2021-08-07T14:03:38.935450Z",
          "iopub.status.idle": "2021-08-07T14:03:38.969882Z",
          "shell.execute_reply": "2021-08-07T14:03:38.969106Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.935882Z"
        },
        "id": "6W9ZUG-14pI_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
        "    )\n",
        "\n",
        "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    logger.info(\"*** Running trainng, Num examples = %d, Num Epochs = %d ***\", len(train_dataset), args.num_train_epochs)\n",
        "\n",
        "    global_step, epochs_trained = 0, 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproducibility\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            \n",
        "            inputs, labels = (batch, batch)\n",
        "            if inputs.shape[1] > 1024: continue\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "    tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWjTu6fI4yP8"
      },
      "source": [
        "# Main Runner\n",
        "\n",
        "Here we're simply setting up the logger and starting the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:03:38.971807Z",
          "iopub.status.busy": "2021-08-07T14:03:38.971342Z",
          "iopub.status.idle": "2021-08-07T14:03:38.987706Z",
          "shell.execute_reply": "2021-08-07T14:03:38.986681Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.971770Z"
        },
        "id": "Jludg4aN4zdc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def main(df_trn):\n",
        "    args = Args()\n",
        "    \n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    device = torch.device(\"cuda\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s\", args.local_rank, device, args.n_gpu)\n",
        "\n",
        "    set_seed(args) # Set seed\n",
        "\n",
        "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
        "    model = AutoModelWithLMHead.from_pretrained(args.model_name_or_path, from_tf=False, config=config, cache_dir=args.cache_dir)\n",
        "    model.to(args.device)\n",
        "    \n",
        "    # Training\n",
        "    train_dataset = load_and_cache_examples(args, tokenizer, df_trn)\n",
        "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "    model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "    # Good practice: save your training arguments together with the trained model\n",
        "    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "    # Load a trained model and vocabulary that you have fine-tuned\n",
        "    model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "    model.to(args.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApbF-p305CYv"
      },
      "source": [
        "# Lets Run it!\n",
        "This should take around 10 minutes so you might as well go grab a cup of coffee ‚òïÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:03:38.989811Z",
          "iopub.status.busy": "2021-08-07T14:03:38.989335Z",
          "iopub.status.idle": "2021-08-07T14:08:00.933974Z",
          "shell.execute_reply": "2021-08-07T14:08:00.933072Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.989772Z"
        },
        "id": "sfTdpQy-5D1n",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10/03/2022 16:23:19 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1\n",
            "10/03/2022 16:23:19 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/config.json from cache at cached/8833f19dc2d2e7283d03d94f879e592e5512320f1a1f2c02f0365e8083441740.92596f8e95f12e3301753009c5c280b3d6e4f5861fcda63c97cf496529703153\n",
            "10/03/2022 16:23:19 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"conversational\": {\n",
            "      \"max_length\": 1000\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/03/2022 16:23:20 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/config.json from cache at cached/8833f19dc2d2e7283d03d94f879e592e5512320f1a1f2c02f0365e8083441740.92596f8e95f12e3301753009c5c280b3d6e4f5861fcda63c97cf496529703153\n",
            "10/03/2022 16:23:20 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"conversational\": {\n",
            "      \"max_length\": 1000\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/03/2022 16:23:20 - INFO - transformers.tokenization_utils -   Model name 'microsoft/DialoGPT-medium' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'microsoft/DialoGPT-medium' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "10/03/2022 16:23:20 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/vocab.json from cache at cached/160770c7e6068191582afd6748b1f7e8395a4e6e63264fa390d534c6e25184b9.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "10/03/2022 16:23:20 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/merges.txt from cache at cached/2768fc6cab7211630a47d239a3c467e01b5edcc650491f3777f181979ed61486.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "10/03/2022 16:23:20 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/added_tokens.json from cache at None\n",
            "10/03/2022 16:23:20 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/special_tokens_map.json from cache at None\n",
            "10/03/2022 16:23:20 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/tokenizer_config.json from cache at None\n",
            "10/03/2022 16:23:21 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/microsoft/DialoGPT-medium/pytorch_model.bin from cache at cached/dfce0c30714b109db92d119a65f60c177b0307b001aaeab3acb46baa6fd83caf.d62bc4460f435335df940faeff855fa04937181751e23f4db6ef38919d948abc\n",
            "10/03/2022 16:23:28 - INFO - transformers.modeling_utils -   Weights of GPT2LMHeadModel not initialized from pretrained model: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.masked_bias']\n",
            "10/03/2022 16:23:41 - INFO - __main__ -   Creating features from dataset file at cached\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load and cache examples is being run ****************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10/03/2022 16:23:43 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
            "10/03/2022 16:23:43 - INFO - __main__ -   *** Running trainng, Num examples = 1898, Num Epochs = 3 ***\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d807ea054f7413c84465ff68ffea244",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e305124863849b2b05fa622206662a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/474 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/localscratch/chenboc1/anaconda3/envs/py36/lib/python3.6/site-packages/transformers/modeling_gpt2.py:148: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272155627/work/aten/src/ATen/native/TensorCompare.cpp:328.)\n",
            "  w = torch.where(mask, w, self.masked_bias)\n",
            "/localscratch/chenboc1/anaconda3/envs/py36/lib/python3.6/site-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272155627/work/torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0ae0f7c29c7418e8fa7c9fd3caa0c5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/474 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ba002aba9334415bda28fef12917499",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/474 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/localscratch/chenboc1/anaconda3/envs/py36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "10/03/2022 16:27:45 - INFO - __main__ -    global_step = 1422, average loss = 1.1955167309141361\n",
            "10/03/2022 16:27:45 - INFO - __main__ -   Saving model checkpoint to output-medium\n",
            "10/03/2022 16:27:45 - INFO - transformers.configuration_utils -   Configuration saved in output-medium/config.json\n",
            "10/03/2022 16:27:47 - INFO - transformers.modeling_utils -   Model weights saved in output-medium/pytorch_model.bin\n",
            "10/03/2022 16:27:48 - INFO - transformers.configuration_utils -   loading configuration file output-medium/config.json\n",
            "10/03/2022 16:27:48 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"conversational\": {\n",
            "      \"max_length\": 1000\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/03/2022 16:27:48 - INFO - transformers.modeling_utils -   loading weights file output-medium/pytorch_model.bin\n",
            "10/03/2022 16:27:55 - INFO - transformers.configuration_utils -   loading configuration file output-medium/config.json\n",
            "10/03/2022 16:27:55 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"conversational\": {\n",
            "      \"max_length\": 1000\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/03/2022 16:27:55 - INFO - transformers.tokenization_utils -   Model name 'output-medium' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'output-medium' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "10/03/2022 16:27:55 - INFO - transformers.tokenization_utils -   Didn't find file output-medium/added_tokens.json. We won't load it.\n",
            "10/03/2022 16:27:55 - INFO - transformers.tokenization_utils -   loading file output-medium/vocab.json\n",
            "10/03/2022 16:27:55 - INFO - transformers.tokenization_utils -   loading file output-medium/merges.txt\n",
            "10/03/2022 16:27:55 - INFO - transformers.tokenization_utils -   loading file None\n",
            "10/03/2022 16:27:55 - INFO - transformers.tokenization_utils -   loading file output-medium/special_tokens_map.json\n",
            "10/03/2022 16:27:55 - INFO - transformers.tokenization_utils -   loading file output-medium/tokenizer_config.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "main(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xYlHoEB5Jic"
      },
      "source": [
        "# Chatting with the trained bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:25:59.805625Z",
          "iopub.status.busy": "2021-08-07T14:25:59.805297Z",
          "iopub.status.idle": "2021-08-07T14:26:35.558955Z",
          "shell.execute_reply": "2021-08-07T14:26:35.557929Z",
          "shell.execute_reply.started": "2021-08-07T14:25:59.805593Z"
        },
        "id": "NkZ0yjsc5LX-",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10/03/2022 16:41:35 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/config.json from cache at /home/chenboc1/.cache/torch/transformers/8833f19dc2d2e7283d03d94f879e592e5512320f1a1f2c02f0365e8083441740.92596f8e95f12e3301753009c5c280b3d6e4f5861fcda63c97cf496529703153\n",
            "10/03/2022 16:41:35 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"conversational\": {\n",
            "      \"max_length\": 1000\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/03/2022 16:41:35 - INFO - transformers.tokenization_utils -   Model name 'microsoft/DialoGPT-medium' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'microsoft/DialoGPT-medium' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "10/03/2022 16:41:36 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/vocab.json from cache at /home/chenboc1/.cache/torch/transformers/160770c7e6068191582afd6748b1f7e8395a4e6e63264fa390d534c6e25184b9.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "10/03/2022 16:41:36 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/merges.txt from cache at /home/chenboc1/.cache/torch/transformers/2768fc6cab7211630a47d239a3c467e01b5edcc650491f3777f181979ed61486.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "10/03/2022 16:41:36 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/added_tokens.json from cache at None\n",
            "10/03/2022 16:41:36 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/special_tokens_map.json from cache at None\n",
            "10/03/2022 16:41:36 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/tokenizer_config.json from cache at None\n",
            "10/03/2022 16:41:36 - INFO - transformers.configuration_utils -   loading configuration file runs/output-medium/config.json\n",
            "10/03/2022 16:41:36 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"conversational\": {\n",
            "      \"max_length\": 1000\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/03/2022 16:41:36 - INFO - transformers.modeling_utils -   loading weights file runs/output-medium/pytorch_model.bin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type \"q\" to quit. Automatically quits after 5 messages\n",
            "DialoGPT: Hiya.\n",
            "DialoGPT: Fine. How are you?\n",
            "DialoGPT: Chris.\n",
            "DialoGPT: Sure! Gimme a minute.\n",
            "DialoGPT: !!!!!!!!!!!!!!!!!!!!!!!!!?!!!,!!?!!...!!.!!..!!\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(f'microsoft/DialoGPT-{model_size}')\n",
        "model = AutoModelWithLMHead.from_pretrained(f'runs/output-{model_size}')\n",
        "chat(model, tokenizer, trained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vFiJlFZwuPF"
      },
      "source": [
        "That's more like it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZVKRKULw1OM"
      },
      "source": [
        "# Uploading to HuggingFace ü§ó\n",
        "\n",
        "HuggingFace is a platform for hosting machine learning models. Think Github for ML. \n",
        "\n",
        "We're going to use it to host our new bot so that it can be accessed from anywhere, even after this google colab session ends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:26:35.560818Z",
          "iopub.status.busy": "2021-08-07T14:26:35.560456Z",
          "iopub.status.idle": "2021-08-07T14:26:36.281001Z",
          "shell.execute_reply": "2021-08-07T14:26:36.279937Z",
          "shell.execute_reply.started": "2021-08-07T14:26:35.560779Z"
        },
        "id": "fVnt8HGS3XwY",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sudo] password for chenboc1: \n",
            "[sudo] password for chenboc1: "
          ]
        }
      ],
      "source": [
        "! apt-get install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ifVP6WroFib"
      },
      "source": [
        "Git needs an email address:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-07T14:26:36.284984Z",
          "iopub.status.busy": "2021-08-07T14:26:36.284686Z",
          "iopub.status.idle": "2021-08-07T14:26:48.292840Z",
          "shell.execute_reply": "2021-08-07T14:26:48.290940Z",
          "shell.execute_reply.started": "2021-08-07T14:26:36.284948Z"
        },
        "id": "Hl21ldCcnDIy",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mPython ('adv_conv') ÈúÄË¶ÅÂÆâË£Ö ipykernel„ÄÇ\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n adv_conv ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "userEmail = input(\"Enter git email: \")\n",
        "!git config --global user.email \"$userEmail\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y50Xqym82bx8"
      },
      "source": [
        "Login with your huggingface account, if you don't have one you can sign up [here](https://huggingface.co/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.294269Z",
          "iopub.status.idle": "2021-08-07T14:26:48.294929Z"
        },
        "id": "7z4uYXxO2U9W",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mPython ('adv_conv') ÈúÄË¶ÅÂÆâË£Ö ipykernel„ÄÇ\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n adv_conv ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.296320Z",
          "iopub.status.idle": "2021-08-07T14:26:48.296950Z"
        },
        "id": "JT9O8EkD2svv",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mPython ('adv_conv') ÈúÄË¶ÅÂÆâË£Ö ipykernel„ÄÇ\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n adv_conv ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "model_name = \"RickBot\" # you can change this, make sure it doesn't contain any spaces though\n",
        "\n",
        "conversational_tag = \"\"\"---\n",
        "tags:\n",
        "- conversational\n",
        "---\n",
        "# RickBot built for [Chai](https://chai.ml/)\n",
        "Make your own [here](https://colab.research.google.com/drive/1o5LxBspm-C28HQvXN-PRQavapDbm5WjG?usp=sharing)\"\"\"\n",
        "\n",
        "model.push_to_hub(model_name)\n",
        "! echo \"$conversational_tag\" > \"$model_name/README.md\"\n",
        "tokenizer.push_to_hub(model_name)\n",
        "\n",
        "! rm -r \"$model_name/\"   # clean up local directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfBpCJdWgoT4"
      },
      "source": [
        "You can copy and paste the url above into your browser to see your new bot's page on huggingface\n",
        "\n",
        "Great! Now our bot is being hosted on HuggingFace we can deploy to Chai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx6TdIB1w3E6"
      },
      "source": [
        "# Deploying to Chai\n",
        "\n",
        "<img src=\"https://i.imgur.com/IjZ12pt.png\" width=\"500\">\n",
        "\n",
        "Chai is a platform for creating, sharing and interacting with conversational AI's. It allows us to chat with our new bot through a mobile app. This means you can show it off really easily, no need to whip out your laptop and fire up a colab instance, simply open the app and get chatting!\n",
        "\n",
        "There is also a bot leaderboard to climb. We can see how our new bot compares to others on the platform:\n",
        "\n",
        "<img src=\"https://i.imgur.com/ctPYQVZ.png\" width=\"850\">\n",
        "\n",
        "Lets deploy RickBot to Chai!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5_ve-Mrca7f"
      },
      "source": [
        "Install the chaipi package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.298101Z",
          "iopub.status.idle": "2021-08-07T14:26:48.298705Z"
        },
        "id": "TiQIVJJwcHfi",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement chaipy (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for chaipy\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! /localscratch/chenboc1/anaconda3/envs/py36/bin/pip install --upgrade chaipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLBVX5HQchDd"
      },
      "source": [
        "Setup the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.299860Z",
          "iopub.status.idle": "2021-08-07T14:26:48.300584Z"
        },
        "id": "4IYwxLmfcaMO",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'chai_py'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c796a16f2649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mchai_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchai_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chai_py'"
          ]
        }
      ],
      "source": [
        "import chai_py\n",
        "chai_py.setup_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlaFXRsjckgg"
      },
      "source": [
        "Head over to the [Chai Dev Platform](https://chai.ml/dev/) to set up your developer account. This allows us to deploy the bot under our own account\n",
        "\n",
        "Your developer ID and keys can be found on the [dev page](https://chai.ml/dev/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.301772Z",
          "iopub.status.idle": "2021-08-07T14:26:48.302342Z"
        },
        "id": "i7Lzej2nc1_1",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mPython ('adv_conv') ÈúÄË¶ÅÂÆâË£Ö ipykernel„ÄÇ\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n adv_conv ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from chai_py.auth import set_auth\n",
        "\n",
        "DEV_UID = input(\"Enter dev UID: \")\n",
        "DEV_KEY = input(\"Enter dev key: \")\n",
        "set_auth(DEV_UID, DEV_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxs9aQWiQUE5"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mPython ('adv_conv') ÈúÄË¶ÅÂÆâË£Ö ipykernel„ÄÇ\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n adv_conv ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "user_name = !huggingface-cli whoami\n",
        "userPlusModel = f\"{user_name[0]}/{model_name}\"\n",
        "# ! echo $userPlusModel > bot/myArguments.txt\n",
        "%store userPlusModel >bot/myArguments.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzKdzMVSe0PH"
      },
      "source": [
        "### Chai bot code\n",
        "\n",
        "You can add to the `past_user_inputs` to change the bot's starting context. This changes how the bot will act when the conversation starts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.305360Z",
          "iopub.status.idle": "2021-08-07T14:26:48.305954Z"
        },
        "id": "F3ktc9LUe-F-",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mPython ('adv_conv') ÈúÄË¶ÅÂÆâË£Ö ipykernel„ÄÇ\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n adv_conv ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "%%write_and_run bot bot.py Bot\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from chai_py import ChaiBot, Update\n",
        "\n",
        "try:\n",
        "  f = open(\"myArguments.txt\", \"r\")\n",
        "except:\n",
        "  f = open(\"bot/myArguments.txt\", \"r\")\n",
        "userPlusModel = f.read()\n",
        "\n",
        "class Bot(ChaiBot):\n",
        "    \n",
        "    def setup(self):\n",
        "        self.ENDPOINT = f\"https://api-inference.huggingface.co/models/{userPlusModel}\"\n",
        "        self.headers = { \"Authorization\": \"Bearer api_oieZbocfGuGxzuQozzaqpFYnBrpBsSLwzP\" }\n",
        "        self.first_response = \"Hey, I'm Rick\" # you can change this\n",
        "\n",
        "    async def on_message(self, update: Update) -> str:\n",
        "        if update.latest_message.text == self.FIRST_MESSAGE_STRING:\n",
        "            return self.first_response\n",
        "        payload = await self.get_payload(update)\n",
        "        return self.query(payload)\n",
        "\n",
        "    def query(self, payload):\n",
        "        data = json.dumps(payload)\n",
        "        response = requests.post(self.ENDPOINT, headers=self.headers, data=data)\n",
        "\n",
        "        if (response.status_code == 503):  # This means we need to wait for the model to load üò¥.\n",
        "            estimated_time = response.json()[\"estimated_time\"]\n",
        "            time.sleep(estimated_time)\n",
        "            data = json.loads(data)\n",
        "            data[\"options\"] = {\"use_cache\": False, \"wait_for_model\": True}\n",
        "            data = json.dumps(data)\n",
        "            response = requests.post(self.ENDPOINT, headers=self.headers, data=data)\n",
        "\n",
        "        return json.loads(response.content.decode(\"utf-8\"))[\"generated_text\"]\n",
        "\n",
        "    async def get_payload(self, update):\n",
        "        past_user_inputs = [\"Hey\"]  # You can add to this!\n",
        "        generated_responses = [self.first_response]  # and this!\n",
        "        return {\n",
        "            \"inputs\": {\n",
        "                \"past_user_inputs\": past_user_inputs,\n",
        "                \"generated_responses\": generated_responses,\n",
        "                \"text\": update.latest_message.text,\n",
        "            },\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ADjwIDlEXT"
      },
      "source": [
        "## Deploy to Chai\n",
        "Time to deploy the bot!\n",
        "\n",
        "You can change the `image_url` and `description` to personalise how the bot will appear on the platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.307041Z",
          "iopub.status.idle": "2021-08-07T14:26:48.307661Z"
        },
        "id": "u1qprkX3lJFs",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mPython ('adv_conv') ÈúÄË¶ÅÂÆâË£Ö ipykernel„ÄÇ\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: 'conda install -n adv_conv ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from chai_py import package, Metadata, upload_and_deploy, wait_for_deployment, share_bot\n",
        "\n",
        "package(\n",
        "    Metadata(\n",
        "        name=model_name,\n",
        "        image_url=\"https://live.staticflickr.com/65535/48185490292_1896035611_b.jpg\",\n",
        "        color=\"0000ff\",\n",
        "        description=\"Pickle Rick!\",\n",
        "        input_class=Bot,\n",
        "        developer_uid=DEV_UID,\n",
        "        memory=3000,\n",
        "    )\n",
        ")\n",
        "\n",
        "print()\n",
        "bot_uid = upload_and_deploy(\"bot/_package.zip\")\n",
        "wait_for_deployment(bot_uid)\n",
        "share_bot(bot_uid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952b_duKglFn"
      },
      "source": [
        "## Success üéâ\n",
        "Scan the QR code above with your phone and you will be taken to a chat screen with your brand new bot, how cool is that?!\n",
        "\n",
        "(Make sure you have the Chai app installed on your phone)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K4pSp4jg0us"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Any dialogue can be used to train this bot, it just needs to be in the right format. Try making a bot of your favourite film character, or maybe from a TV show.\n",
        "\n",
        "You can also try changing the training config. For example, the context length (`n`), or any of the arguments in the `Args` class."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.6.13 ('py36')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "30da4dc233a7d49a4e947855a02725646840a9e32fd52ff96776e60e14cf77ab"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
